{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words classification of STL-10 dataset with HOG features and nearest-neighbor classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import ndimage, spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define main parameters: path to STL-10, list of names for considered classes, number of codebook words (i.e. K-means clusters), type of norm for determining nearest neighbor of BoW histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/home/cvcourse/pics/STL-10/images_per_class'\n",
    "class_names_input = ['cat', 'ship']\n",
    "K = 100\n",
    "nearest_neighbor_norm = 'L2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter specified class names to obtain a valid subset of STL-10 classes. If this subset has less than two elements, exit with an error status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STL10_class_names = ['airplane', 'bird', 'car', 'cat', 'deer', 'dog', 'horse', 'monkey', 'ship', 'truck']\n",
    "class_names_input_unique = np.unique(class_names_input)\n",
    "is_input_valid = np.array([c in STL10_class_names for c in class_names_input_unique])\n",
    "class_names = class_names_input_unique[is_input_valid]\n",
    "C = len(class_names)\n",
    "if C < 2:\n",
    "    print('Not enough classes to distinguish. Need at least 2 classes from STL-10!')\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for extraction of HOG features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_of_feature_points(image, n_points_x, n_points_y, margin_x, margin_y):\n",
    "    \"\"\"\n",
    "    Construct grid of feature points to serve as patch centers for computation of HOG features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Return the set of feature points as two 1D arrays holding their image coordinates.\n",
    "    return feature_points_x, feature_points_y\n",
    "\n",
    "\n",
    "def compute_HOG_descriptors(image, feature_points_x, feature_points_y, cell_width, cell_height):\n",
    "    \"\"\"\n",
    "    Compute the HOG descriptors, as the set of features for an input image, at the specified points.\n",
    "    Output:\n",
    "        |HOG_descriptors|: 2D NumPy array of shape (n_points, n_cells * n_cells * n_bins)\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters and constants.\n",
    "    n_bins = 8\n",
    "    n_points = feature_points_x.shape[0]\n",
    "    n_cells = 4\n",
    "    pi = np.pi\n",
    "\n",
    "    return HOG_descriptors\n",
    "\n",
    "\n",
    "def feature_extraction(image_full_filename):\n",
    "    \"\"\"\n",
    "    Extract HOG features for an input image.\n",
    "    Inputs:\n",
    "        |image_full_filename|: full path to the input image file\n",
    "    Output:\n",
    "        2D NumPy array of shape (n_points_x * n_points_y, 128)\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the input image into a numpy.ndarray variable of two dimensions (grayscale) for further processing.\n",
    "    image = cv2.imread(image_full_filename, 0).astype('float')\n",
    "\n",
    "    # Define parameters.\n",
    "    n_points_x = 6\n",
    "    n_points_y = 6\n",
    "    cell_width = 4\n",
    "    cell_height = 4\n",
    "    margin_x = 2 * cell_width\n",
    "    margin_y = 2 * cell_height\n",
    "\n",
    "    # Construct grid of feature points.\n",
    "    feature_points_x, feature_points_y = grid_of_feature_points(image, n_points_x, n_points_y, margin_x, margin_y)\n",
    "\n",
    "    # Return HOG features at the computed feature points.\n",
    "    return compute_HOG_descriptors(image, feature_points_x, feature_points_y, cell_width, cell_height)\n",
    "\n",
    "\n",
    "def image_full_filenames_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Return a list with full filenames of all images in the input directory, sorted in lexicographical order.\n",
    "    Inputs:\n",
    "        |directory|: path to input directory.\n",
    "    \"\"\"\n",
    "\n",
    "    image_format = '.png'\n",
    "    image_filename_pattern = os.path.join(directory, '*' + image_format)\n",
    "    list_image_full_filenames = glob.glob(image_filename_pattern)\n",
    "    # Sort the list.\n",
    "    list_image_full_filenames = sorted(list_image_full_filenames)\n",
    "\n",
    "    return list_image_full_filenames\n",
    "\n",
    "\n",
    "def class_features(class_directory):\n",
    "    \"\"\"\n",
    "    Construct a 3D numpy.ndarray holding the HOG features for all images in a class, under the input directory.\n",
    "    Inputs:\n",
    "        |class_directory|: path to input directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the list with all images in the class directory.\n",
    "    list_image_full_filenames = image_full_filenames_in_directory(class_directory)\n",
    "    n_images = len(list_image_full_filenames)\n",
    "\n",
    "    # Initialize a list of HOG features per image.\n",
    "    HOG_features = []\n",
    "\n",
    "    # Main loop over the images to compute and append HOG features.\n",
    "    for i in range(n_images):\n",
    "        # Display progress.\n",
    "        print('Feature extraction for image {:d}/{:d}'.format(i + 1, n_images))\n",
    "\n",
    "        # Extract features for current image as a 2D numpy.ndarray and append it to the list.\n",
    "        HOG_features.append(feature_extraction(list_image_full_filenames[i]))\n",
    "\n",
    "    # Concatenate feature vectors from all images into a single 3D numpy.ndarray with dimensions\n",
    "    # n_images-by-n_descriptors-by-D.\n",
    "    # ASSUMPTION: all images of processed classes have equal dimensions, therefore equal n_points for the constructed\n",
    "    # grids.\n",
    "    HOG_features_class = np.array(HOG_features)\n",
    "\n",
    "    return HOG_features_class\n",
    "\n",
    "\n",
    "def split_features(dataset_dir, split, class_names):\n",
    "    \"\"\"\n",
    "    Construct a list of 3D arrays, one for each class, with features for an entire split of the dataset.\n",
    "    Inputs:\n",
    "        |dataset_dir|: path to root dataset directory.\n",
    "        |split|: name of processed split, e.g. 'train' or 'test'.\n",
    "        |class_names|: list of names of considered classes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Form path to root split directory.\n",
    "    split_dir = os.path.join(dataset_dir, split)\n",
    "\n",
    "    HOG_features_split = []\n",
    "\n",
    "    # Main loop over classes.\n",
    "    for i in range(len(class_names)):\n",
    "        current_class_name = class_names[i]\n",
    "\n",
    "        # Display progress.\n",
    "        print('Processing {:s} split, class {:d}: {:s}'.format(split, i + 1, current_class_name))\n",
    "\n",
    "        # Extract features.\n",
    "        HOG_features_split.append(class_features(os.path.join(split_dir, current_class_name)))\n",
    "\n",
    "    return HOG_features_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training** and **testing** - **step 1)** compute HOG features for the entire train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 'train'\n",
    "HOG_features_train = split_features(dataset_dir, train_split, class_names)\n",
    "\n",
    "# Concatenate HOG features from all classes of the train split into one 2D matrix.\n",
    "n_images_per_class, n_descriptors_per_image, D = HOG_features_train[0].shape\n",
    "HOG_features_train_concatenated = np.empty((0, D))\n",
    "for c in range(C):\n",
    "    HOG_features_train_concatenated = np.concatenate((HOG_features_train_concatenated,\n",
    "                                                      np.reshape(HOG_features_train[c], (-1, D))))\n",
    "\n",
    "test_split = 'test'\n",
    "HOG_features_test = split_features(dataset_dir, test_split, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for codebook construction via K-means clustering, Bag-of-Words histogram representation, nearest-neighbor classification, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbor_L2(points_1, points_2):\n",
    "    \"\"\"\n",
    "    Determine the nearest neighbor of each point of the first set from the second set in the L2-norm sense.\n",
    "    Inputs:\n",
    "        |points_1|: 2D numpy.ndarray containing the first set of points, with dimensions N-by-D.\n",
    "        |points_2|: 2D numpy.ndarray containing the second set of points, with dimensions K-by-D.\n",
    "    Output:\n",
    "        1D NumPy array with N elements, corresponding to the indices of points in |points_2| that are the nearest\n",
    "        neighbors of points in |points_1|\n",
    "    \"\"\"\n",
    "\n",
    "    return nearest_neighbor_indices\n",
    "\n",
    "\n",
    "def kmeans(points, K, n_iter):\n",
    "    \"\"\"\n",
    "    Cluster the input points into K clusters using K-means with the specified number of iterations and output the\n",
    "    induced cluster centroids.\n",
    "    Inputs:\n",
    "        |points|: 2D numpy.ndarray containing feature vectors as its rows, with dimensions N-by-D\n",
    "        |K|: number of clusters\n",
    "        |n_iter|: number of iterations of K-means algorithm\n",
    "    Output:\n",
    "        |centroids|: 2D numpy.ndarray containing the final cluster centroids as its rows, with dimensions K-by-D\n",
    "    \"\"\"\n",
    "\n",
    "    N, n_dims = points.shape[:2]\n",
    "\n",
    "    # Centroid initialization with randomly selected feature vectors.\n",
    "    # centroids = ...\n",
    "\n",
    "    # Main K-means loop.\n",
    "    for i in range(n_iter):\n",
    "        # 1) Cluster assignment.\n",
    "\n",
    "        # 2) Centroid update based on current assignment.\n",
    "        for k in range(K):\n",
    "            # Check if cluster is empty.\n",
    "\n",
    "        # Display progress.\n",
    "        print('Completed K-means iteration {:d}/{:d}'.format(i+1, n_iter))\n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "def bow_histograms_and_labels(HOG_features_split, codebook_words):\n",
    "    \"\"\"\n",
    "    Compute the Bag-of-Words histograms for an entire split of the dataset, using the respective codebook with visual\n",
    "    words that has been computed with K-means. Also create an array of ground truth labels for images in the split.\n",
    "    Inputs:\n",
    "        |HOG_features_split|: list of 3D arrays, one for each class, in which each array holds the features for all\n",
    "        images in the split that belong to that class\n",
    "        |codebook_words|: 2D numpy.ndarray containing codebook words as its rows, with dimensions K-by-D\n",
    "    \"\"\"\n",
    "\n",
    "    C = len(HOG_features_split)\n",
    "    K, D = codebook_words.shape\n",
    "\n",
    "    # Initialize matrix of BoW histograms and array of ground truth labels.\n",
    "    bow_histograms_split = np.empty((0, K))\n",
    "    labels_split = np.empty((0, 1), dtype=int)\n",
    "\n",
    "    for c in range(C):\n",
    "        HOG_features_class = HOG_features_split[c]\n",
    "        n_images = HOG_features_class.shape[0]\n",
    "\n",
    "        # Add labels of current class to overall label array.\n",
    "        labels_split = np.concatenate((labels_split, c + np.zeros((n_images, 1), dtype=int)))\n",
    "\n",
    "        # Initializations.\n",
    "        bow_histograms_class = np.zeros((n_images, K))\n",
    "\n",
    "        # Loop over all images in the class and compute BoW histograms.\n",
    "        for i in range(n_images):\n",
    "            # |HOG_features_image| is a 2D numpy.ndarray containing all HOG descriptors of the current image as its rows.\n",
    "            HOG_features_image = HOG_features_class[i]\n",
    "            # Assign each descriptor of the current image to a word.\n",
    "            # ...\n",
    "            # Count how many descriptors are assigned to each word.\n",
    "            # bow_histograms_class[i, :] = ...\n",
    "\n",
    "        # Append BoW histograms for images in current class to the overall split-level matrix.\n",
    "        bow_histograms_split = np.concatenate((bow_histograms_split, bow_histograms_class))\n",
    "\n",
    "    return bow_histograms_split, labels_split\n",
    "\n",
    "\n",
    "def nearest_neighbor_classifier(points_test, points_train, labels_train, norm='L2'):\n",
    "    \"\"\"\n",
    "    Classify test points by assigning to each of them the label of its nearest neighbor point from the training set.\n",
    "    Inputs:\n",
    "        |points_test|: 2D numpy.ndarray containing the test points as its rows, with dimensions S-by-K.\n",
    "        |points_train|: 2D numpy.ndarray containing the train points as its rows, with dimensions T-by-K.\n",
    "        |labels_train|: 1D numpy.ndarray containing the ground truth labels of the train points, with dimensions T-by-1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute nearest neighbors.\n",
    "    if norm == 'L2':\n",
    "        # ...\n",
    "    else:\n",
    "        # ...\n",
    "\n",
    "    # Assign to test points the label of their nearest training neighbor.\n",
    "    # labels_test = ...\n",
    "\n",
    "    return labels_test\n",
    "\n",
    "\n",
    "def confusion_matrix(labels_ground_truth, labels_predicted, C):\n",
    "    \"\"\"\n",
    "    Compute the confusion matrix based on the ground truth labels and the respective predictions.\n",
    "    Inputs:\n",
    "        |labels_ground_truth|: 1D numpy.ndarray containing the ground truth labels, with dimensions S-by-1.\n",
    "        |labels_predicted|: 1D numpy.ndarray containing the predicted labels, with same dimensions as\n",
    "                            |labels_ground_truth|.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize confusion matrix to zero values.\n",
    "    conf = np.zeros((C, C))\n",
    "\n",
    "    # Use definition of confusion matrix to compute its values: rows correspond to ground truth labels, columns to\n",
    "    # predictions.\n",
    "    np.add.at(conf, (labels_ground_truth, labels_predicted), 1)\n",
    "\n",
    "    return conf\n",
    "\n",
    "\n",
    "def accuracy_from_confusion_matrix(conf):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of a classifier from the confusion matrix related to its predictions.\n",
    "    Input:\n",
    "        |conf|: confusion matrix as a 2D numpy.ndarray, with dimensions C-by-C.\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.trace(conf) / np.sum(conf)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training** and **testing** - **step 2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_evaluation_rounds = 10\n",
    "\n",
    "# Initialize confusion matrices and array of accuracy values.\n",
    "confusion_matrices = np.zeros((n_evaluation_rounds, C, C))\n",
    "accuracy_values = np.zeros(n_evaluation_rounds)\n",
    "\n",
    "# Fix random seed to ensure reproducibility of the results.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define other parameters.\n",
    "n_iters_kmeans = 10\n",
    "\n",
    "# Main loop to repeat training and testing.\n",
    "for i in range(n_evaluation_rounds):\n",
    "\n",
    "    print('Running {:d}/{:d} evaluation round for Bag-of-Words classification'.format(i+1, n_evaluation_rounds))\n",
    "\n",
    "    # TRAINING - STEP 2)i) Construct the codebook of HOG feature vectors by applying K-means to the entire set of\n",
    "    # training features.\n",
    "    print('Constructing codebook from training features using K-means...')\n",
    "    codebook_words = kmeans(HOG_features_train_concatenated, K, n_iters_kmeans)\n",
    "    print('Codebook constructed.')\n",
    "\n",
    "    # TRAINING - STEP 2)ii) Compute the Bag-of-Words histogram representation of all training images that is induced\n",
    "    # by the constructed codebook.\n",
    "    bow_histograms_train, labels_train = bow_histograms_and_labels(HOG_features_train, codebook_words)\n",
    "\n",
    "    # TESTING - STEP 2)i) Compute the Bag-of-Words histogram representation of all testing images that is induced\n",
    "    # by the constructed codebook.\n",
    "    bow_histograms_test, labels_test_ground_truth = bow_histograms_and_labels(HOG_features_test, codebook_words)\n",
    "\n",
    "    # TESTING - STEP 2)ii) Predict test labels with nearest-neighbor classifier.\n",
    "    labels_test_predicted = nearest_neighbor_classifier(bow_histograms_test, bow_histograms_train, labels_train,\n",
    "                                                        nearest_neighbor_norm)\n",
    "\n",
    "    # TESTING - STEP 2)iii) Evaluate the predictions of the classifier on the test split against ground truth.\n",
    "    confusion_matrices[i] = confusion_matrix(labels_test_ground_truth, labels_test_predicted, C)\n",
    "    accuracy_values[i] = accuracy_from_confusion_matrix(confusion_matrices[i])\n",
    "\n",
    "# Report cumulative results over all evaluation rounds.\n",
    "accuracy_average = np.mean(accuracy_values)\n",
    "accuracy_std = np.std(accuracy_values, ddof=1)\n",
    "print('%%%%%%%%%%%%%%%%%%%%%%%%%\\n\\n')\n",
    "print('Average BoW classification accuracy over {:d} rounds: {:6.2f}% +/- {:5.2f}%'.format(n_evaluation_rounds,\n",
    "                                                                                           100 * accuracy_average,\n",
    "                                                                                           100 * (3 * accuracy_std)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
