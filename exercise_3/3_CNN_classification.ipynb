{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhFion25g2uX"
   },
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# import required modules\n",
    "# ===========================================================================\n",
    "import os\n",
    "\n",
    "# ===================\n",
    "# for array manipulations, etc.\n",
    "# ===================\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# ===================\n",
    "# for visualization\n",
    "# ===================\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# ===================\n",
    "# pytorch for CNN related things\n",
    "# ===================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set path to the STL dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STL_path = '/home/cvcourse/pics/stl10_binary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset (images + labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "oi61lBJoZ3Dt",
    "outputId": "596dce4c-ef9b-4dc5-efed-161855213f5f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ===================\n",
    "# function to load the dataset and format the data as required for further processing \n",
    "# ===================\n",
    "# inputs:\n",
    "#    split: string denoting whether the train or the test data is to be loaded \n",
    "# outputs:\n",
    "#    X: dataset inputs (images)\n",
    "#    y: dataset outputs (labels for the images)\n",
    "# ===================\n",
    "def load_split(dataset_root, split):\n",
    "    X_path = '{}/{}_X.bin'.format(dataset_root, split)\n",
    "    y_path = '{}/{}_y.bin'.format(dataset_root, split)\n",
    "    X = np.fromfile(X_path, dtype=np.uint8).reshape((-1, 3, 96, 96))\n",
    "    X = np.moveaxis(X, 2, 3) # convert column-major to row-major\n",
    "    y = np.fromfile(y_path, dtype=np.uint8) # data type seems to be important here...\n",
    "    y = y - 1 #convert range [1,10] to [0,9]\n",
    "    print('-----------------')\n",
    "    print('Loaded split \"{}\" with sizes: Images {}, labels {}'.format(split, X.shape, y.shape))\n",
    "    return X, y\n",
    "\n",
    "# ===================\n",
    "# load the training and test datasets\n",
    "# ===================\n",
    "full_train_X, full_train_y = load_split(STL_path, 'train')\n",
    "full_test_X, full_test_y = load_split(STL_path, 'test')\n",
    "\n",
    "# ===================\n",
    "# read the names of the classes in the dataset\n",
    "# ===================\n",
    "with open('{}/class_names.txt'.format(STL_path)) as f:\n",
    "    full_class_names = f.readlines()\n",
    "  \n",
    " # ===================\n",
    " # strip trailing whitespace\n",
    " # ===================\n",
    "full_class_names = [name.strip() for name in full_class_names]\n",
    "\n",
    "# ===================\n",
    "# print the list of names of all classes\n",
    "# ===================\n",
    "print('-----------------')\n",
    "print('The classes in the dataset are: ')\n",
    "print(list(enumerate(full_class_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore a few samples of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "20kO0csfaLoM"
   },
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# Explore a few samples of the dataset\n",
    "# (repeat the following cell for different output by pressing Ctrl-Enter on it instead of Shift-Enter)\n",
    "# ===========================================================================\n",
    "\n",
    "# ===================\n",
    "# function to show an example image and its label\n",
    "# ===================\n",
    "# inputs:\n",
    "#    X: image\n",
    "#    y: label\n",
    "#    class_names: list of names of all classes so that the class name corresponding to the label can be identified\n",
    "# outputs:\n",
    "#    None\n",
    "# ===================\n",
    "def visualize_sample(X, y, class_names):\n",
    "    X_vis = np.moveaxis(X, 0, 2) # convert Channel,Width,Height order to W,H,C\n",
    "    plt.figure()\n",
    "    plt.imshow(X_vis, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title('Class id={}, Name={}'.format(y, class_names[y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "oWwJBVCNWuL2",
    "outputId": "ac074aba-eefa-4bac-990e-f864030cee32"
   },
   "outputs": [],
   "source": [
    "# ===================\n",
    "# choose a random image and display it \n",
    "# you may run this cell multiple times to get an idea of the different images in the dataset\n",
    "# alternatively, modify the function 'visualize_sample' to display several (4/8/16) different images along with their labels\n",
    "# ===================\n",
    "i = random.randint(0, full_train_X.shape[0]-1)\n",
    "visualize_sample(full_train_X[i],\n",
    "                 full_train_y[i],\n",
    "                 full_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a 2-class classifier. \n",
    "\n",
    "First we will generate a dataset with just two classes, 'cat' and 'ship'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "AUQ57Sc-hWDt",
    "outputId": "9424649e-7d02-4da9-a4bb-c43459d32c57"
   },
   "outputs": [],
   "source": [
    "# ===================\n",
    "# Let's select from the dataset only samples of classes 'cat' and 'ship'.\n",
    "# We have arbitrarily chosen these two classes.\n",
    "# You may try some other pair of classes if you wish to.\n",
    "# ===================\n",
    "ID_C1_OLD, ID_C2_OLD = 3, 8 # C1: CAT, C2: SHIP\n",
    "ID_C1_NEW, ID_C2_NEW = 0, 1\n",
    "\n",
    "# ===================\n",
    "# function to delete all the classes other than the cat and ship classes\n",
    "# ===================\n",
    "# inputs:\n",
    "#    X: images from all classes\n",
    "#    y: labels from all classes\n",
    "#    split: train / test\n",
    "# outputs:\n",
    "#    X_out: images from only cats and ships\n",
    "#    y_out: labels from only cats and ships\n",
    "# ===================\n",
    "def subsplit_two_classes(X, y, split):\n",
    "    \n",
    "    indices = np.where(np.logical_or(y==ID_C1_OLD,\n",
    "                                   y==ID_C2_OLD))\n",
    "    X_out = X[indices]\n",
    "    y_out = y[indices]\n",
    "    y_out = (y_out == ID_C1_OLD).astype(np.int) * ID_C1_NEW + \\\n",
    "            (y_out == ID_C2_OLD).astype(np.int) * ID_C2_NEW\n",
    "    print('Created \"{}\" subsplit with only 2 classes with sizes: Images {}, labels {}'.format(split, X_out.shape, y_out.shape))\n",
    "    return X_out, y_out\n",
    "\n",
    "# ===================\n",
    "# make the training and test datasets so that they contain only the two classes of interest\n",
    "# ===================\n",
    "train_two_classes_X, train_two_classes_y = subsplit_two_classes(full_train_X,\n",
    "                                                                full_train_y,\n",
    "                                                                'train')\n",
    "\n",
    "test_two_classes_X, test_two_classes_y = subsplit_two_classes(full_test_X,\n",
    "                                                              full_test_y,\n",
    "                                                              'test')\n",
    "\n",
    "class_names_two_classes = [full_class_names[i] for i in (ID_C1_OLD, ID_C2_OLD)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "Rv9JHOKqW1d1",
    "outputId": "f809268f-507c-4a6f-ba7a-5a92b5fa3930"
   },
   "outputs": [],
   "source": [
    "# ===================\n",
    "# visualize some examples from the modified datasets\n",
    "# ===================\n",
    "i = random.randint(0, train_two_classes_X.shape[0]-1)\n",
    "visualize_sample(train_two_classes_X[i],\n",
    "                 train_two_classes_y[i],\n",
    "                 class_names_two_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap our dataset into pytorch Dataset class so it can further be used in DataLoader for batch grouping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHeBj0AduZnU"
   },
   "outputs": [],
   "source": [
    "class ArrayDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "  \n",
    " # ===================\n",
    " # set the batch size here\n",
    " # ===================\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# ===================\n",
    "# load the training and test datasets for the two-class classification problem\n",
    "# https://pytorch.org/docs/stable/data.html\n",
    "# ===================\n",
    "loader_two_classes_train = DataLoader(dataset = ArrayDataset(train_two_classes_X, train_two_classes_y),\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True)\n",
    "\n",
    "loader_two_classes_test = DataLoader(dataset = ArrayDataset(test_two_classes_X, test_two_classes_y),\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=True)\n",
    "\n",
    "# ===================\n",
    "# load the training and test datasets for the ten-class classification problem\n",
    "# ===================\n",
    "loader_ten_classes_train = DataLoader(dataset = ArrayDataset(full_train_X, full_train_y),\n",
    "                                      batch_size=BATCH_SIZE,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True)\n",
    "\n",
    "loader_ten_classes_test = DataLoader(dataset = ArrayDataset(full_test_X, full_test_y),\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "riHvu0CvdVa-"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  \n",
    "  # ===================\n",
    "  # the network architecture is defined here\n",
    "  # documentation of some useful functions:\n",
    "  #     https://pytorch.org/docs/stable/nn.html#conv2d\n",
    "  #     https://pytorch.org/docs/stable/nn.html#batchnorm2d\n",
    "  #     https://pytorch.org/docs/stable/nn.html#maxpool2d\n",
    "  #     https://pytorch.org/docs/stable/nn.html#linear\n",
    "  #     https://pytorch.org/docs/stable/nn.html#batchnorm1d\n",
    "  # ===================\n",
    "    def __init__(self, num_out_classes):\n",
    "    \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        num_filters = [3, 6, 12, 24, 64]\n",
    "    \n",
    "        # expected size of input to following layer: 3x96x96\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_filters[0],\n",
    "                               out_channels=num_filters[1],\n",
    "                               kernel_size=3,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        # expected size of input to following layer:\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=num_filters[1])\n",
    "        # expected size of input to following layer:\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # expected size of input to following layer:\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # expected size of input to following layer:\n",
    "        self.conv2 = nn.Conv2d(in_channels=num_filters[1],\n",
    "                               out_channels=num_filters[2],\n",
    "                               kernel_size=3,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        # expected size of input to following layer:\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=num_filters[2])\n",
    "        # expected size of input to following layer:\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # expected size of input to following layer:\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # expected size of input to following layer:\n",
    "        self.conv3 = nn.Conv2d(in_channels=num_filters[2],\n",
    "                               out_channels=num_filters[3],\n",
    "                               kernel_size=3,\n",
    "                               padding=1,\n",
    "                               bias=False)\n",
    "        # expected size of input to following layer:\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=num_filters[3])\n",
    "        # expected size of input to following layer:\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # expected size of input to following layer:\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # expected size of input to following layer:\n",
    "        self.fc4 = nn.Linear(in_features=num_filters[3]*12*12,\n",
    "                             out_features=num_filters[4])\n",
    "        # expected size of input to following layer:\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=num_filters[4])\n",
    "        # expected size of input to following layer:\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        # expected size of input to following layer:\n",
    "        self.fc5 = nn.Linear(in_features=num_filters[4],\n",
    "                             out_features=num_out_classes)\n",
    "        \n",
    "        # single-layer perceptron neural network\n",
    "        self.perceptron = nn.Linear(in_features=3*96*96,\n",
    "                                    out_features=num_out_classes)\n",
    "\n",
    "    # ===================\n",
    "    # define the forward pass through the network using the operations defined in the init function\n",
    "    # ===================\n",
    "    def forward(self, x):\n",
    "        # bring input to [-1,1] range\n",
    "        x = x.float() / 128.0 - 1.0\n",
    "        # ===================\n",
    "        # sequence of operations: convolution --> batch normalization --> non-linearity --> pooling\n",
    "        # (to be implemented)\n",
    "        # ===================\n",
    "        x = x.view(-1, 3*96*96)\n",
    "        x = self.perceptron(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to compute the accuracy of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJ0v736Z4rUa"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(data_loader,\n",
    "                     cnn,\n",
    "                     split,\n",
    "                     print_output = True):\n",
    "  \n",
    "    cnn.eval() # sets the model into evaluation mode\n",
    "    total_accuracy = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, batch in enumerate(data_loader):\n",
    "\n",
    "        # get the inputs and labels\n",
    "        X_batch, y_batch = batch\n",
    "\n",
    "        # calculate predictions given a batch of samples\n",
    "        predictions_batch = cnn(X_batch)\n",
    "\n",
    "        # convert each prediction into a class id\n",
    "        _, vals = torch.max(predictions_batch, 1)\n",
    "\n",
    "        # calculate the amount of predicted class ids matching the ground truth and increment counters\n",
    "        total_accuracy += (vals == y_batch.type(torch.long)).int().sum()\n",
    "        total_samples += y_batch.shape[0]\n",
    "\n",
    "        out_accuracy = int(total_accuracy) * 1.0 / total_samples\n",
    "\n",
    "    if print_output:\n",
    "        print('Accuracy over \"{}\" split is {:02f}%'.format(split, 100 * out_accuracy))\n",
    "\n",
    "    return 100 * out_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for training the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2HawQWvt7nY"
   },
   "outputs": [],
   "source": [
    "def train(cnn,\n",
    "          loader_train,\n",
    "          loader_test,\n",
    "          num_epochs,\n",
    "          learning_rate = 0.001,\n",
    "          track_accuracy = False):\n",
    "  \n",
    "    # ===================\n",
    "    # define the loss to be minimized\n",
    "    # https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
    "    # ===================\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # ===================\n",
    "    # create an optimizer \n",
    "    # You can read more about optimizers here: https://pytorch.org/docs/stable/optim.html\n",
    "    # You may try with different optimizers and check if that makes any difference\n",
    "    # For any one optimizer, try changing the learning rate and observe the effect.\n",
    "    # ===================\n",
    "    optimizer = optim.SGD(cnn.parameters(),\n",
    "                        lr = learning_rate,\n",
    "                        momentum = 0.9)\n",
    "\n",
    "    # ===================\n",
    "    # set the model into training mode\n",
    "    # ===================\n",
    "    cnn.train() \n",
    "\n",
    "    tr_acc = []\n",
    "    ts_acc = []\n",
    "\n",
    "    # ===================\n",
    "    # In each epoch, run the optimizer operation on each batch\n",
    "    # ===================\n",
    "    for epoch in range(num_epochs):\n",
    "            \n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(loader_train):\n",
    "\n",
    "            X_batch, y_batch = batch\n",
    "            y_batch = y_batch.type(torch.long)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            predictions_batch = cnn(X_batch)\n",
    "\n",
    "            # calculate loss for the provided predictions and ground truth labels\n",
    "            y_batch = torch.max(predictions_batch, 1)[1]\n",
    "            loss = criterion(predictions_batch, y_batch)\n",
    "\n",
    "            epoch_loss += float(loss)\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # update learnable parameters according to gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "        # ===================\n",
    "        # track the training and test accuracy.\n",
    "        # note that this adds quite a bit to the training time,\n",
    "        # as things are running on the cpu currently.\n",
    "        # this is usually a good debugging tool to see how the training is progressing\n",
    "        # and is usually done when people train such networks on gpus.\n",
    "        # ===================\n",
    "        if track_accuracy:\n",
    "            tr_acc.append(compute_accuracy(data_loader = loader_train,\n",
    "                                           cnn = cnn,\n",
    "                                           split = 'train',\n",
    "                                           print_output = False))\n",
    "            ts_acc.append(compute_accuracy(data_loader = loader_test,\n",
    "                                           cnn = cnn,\n",
    "                                           split = 'test',\n",
    "                                           print_output = False))\n",
    "\n",
    "        epoch_loss /= len(loader_train)\n",
    "        print('Epoch {}/{} training loss {:02f}'.format(epoch+1, num_epochs, epoch_loss))\n",
    "\n",
    "    if track_accuracy:\n",
    "        plt.figure()\n",
    "        plt.plot(np.array(tr_acc))\n",
    "        plt.plot(np.array(ts_acc))\n",
    "        plt.legend(['training accuracy', 'test accuracy'])\n",
    "        plt.xlabel('training iterations')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance of the above defined CNN class and compute it's accuracy before training. What do you expect the accuracy to be??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# create an instance of the above defined CNN class\n",
    "# ===========================================================================\n",
    "cnn_two_classes = CNN(num_out_classes = 2)\n",
    "\n",
    "# ===========================================================================\n",
    "# compute the accuracy of the CNN\n",
    "# ===========================================================================\n",
    "compute_accuracy(data_loader = loader_two_classes_train,\n",
    "                 cnn = cnn_two_classes,\n",
    "                 split = 'train')\n",
    "\n",
    "compute_accuracy(data_loader = loader_two_classes_test,\n",
    "                 cnn = cnn_two_classes,\n",
    "                 split = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network now and reevaluate it's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "tTNUehzoO5Uu",
    "outputId": "bdc47eee-85ed-4701-d99f-5a3d6db8116c"
   },
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# call the train function and wait for the magic to unfold ;)\n",
    "# ===========================================================================\n",
    "train(cnn = cnn_two_classes,\n",
    "      loader_train = loader_two_classes_train,\n",
    "      loader_test = loader_two_classes_test,\n",
    "      num_epochs = 10)\n",
    "\n",
    "# ===========================================================================\n",
    "# Let's reevaluate accuracy now. Did the magic happen?\n",
    "# ===========================================================================\n",
    "compute_accuracy(data_loader = loader_two_classes_train,\n",
    "                 cnn = cnn_two_classes,\n",
    "                 split = 'train')\n",
    "\n",
    "compute_accuracy(data_loader = loader_two_classes_test,\n",
    "                 cnn = cnn_two_classes,\n",
    "                 split = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fg-CdoqUN9e"
   },
   "source": [
    "# Multi-class classification\n",
    "\n",
    "\n",
    "\n",
    "The 2-class classification CNN works well. Now, let's try to apply the same approach to multi-class classification problem. Remember that the original dataset has 10 classes. So all that needs to be done is create a new CNN with 10 output classes, and train once again.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yi-ErrUUULx3"
   },
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# create a new instance of the CNN class, to do the classification for all the ten classes\n",
    "# ===========================================================================\n",
    "cnn_ten_classes = CNN(num_out_classes = 10)\n",
    "\n",
    "# ===========================================================================\n",
    "# compute the accuracy of the CNN... we have not yet trained it, so what do you expect the accuracy to be??\n",
    "# ===========================================================================\n",
    "compute_accuracy(data_loader = loader_ten_classes_train,\n",
    "                 cnn = cnn_ten_classes,\n",
    "                 split = 'train')\n",
    "\n",
    "compute_accuracy(data_loader = loader_ten_classes_test,\n",
    "                 cnn = cnn_ten_classes,\n",
    "                 split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "colab_type": "code",
    "id": "OAajqGfPUmKD",
    "outputId": "cf5115a6-3b33-48ad-888b-fce0ddad8d50"
   },
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# train this new CNN...\n",
    "# ===========================================================================\n",
    "train(cnn = cnn_ten_classes,\n",
    "      loader_train = loader_ten_classes_train,\n",
    "      loader_test = loader_ten_classes_test,\n",
    "      num_epochs = 10)\n",
    "\n",
    "# ===========================================================================\n",
    "# compute the accuracy with the trained CNN...\n",
    "# ===========================================================================\n",
    "compute_accuracy(data_loader = loader_ten_classes_train,\n",
    "                 cnn = cnn_ten_classes,\n",
    "                 split = 'train')\n",
    "\n",
    "compute_accuracy(data_loader = loader_ten_classes_test,\n",
    "                 cnn = cnn_ten_classes,\n",
    "                 split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WhW5HTrzXJ3y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of BIWI_CNN_hands_on_2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
